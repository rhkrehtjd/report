{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f87534-320f-452f-aa86-8c29a64f6280",
   "metadata": {},
   "source": [
    "## 01 사이킷런(scikit-learn)\n",
    " - 다양한 머신러닝 알고리즘을 구현한 파이썬 라이브러리\n",
    " - 다양하고 쉽고 효율적인 프레임워크와 API 제공\n",
    " - 오랜 기간 실전 환경에서 검증 되었으며, 많이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c601f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit_learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc77674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed23dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0dbde6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 02 붓꽃 품종 예측\n",
    "\n",
    "**붓꽃 데이터 세트 (꽃잎의 길이와 너비, 꽃받침의 길이와 너비)를 이용하여 붓꽃의 품종을 분류**\n",
    "\n",
    "- sklearn.datasets: 사이킷런에서 자체적으로 제공하는 데이터 세트 생성하는 모듈의 모임\n",
    "- sklearn.tree: 트리 기반 ML 알고리즘 구현 클래스 모임\n",
    "- sklearn.model_selection: 학습 데이터와 검증데이터, 예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 모듈의 모임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b97728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0cd54b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8c4b6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'C:\\\\Users\\\\USER\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#붓꽃 데이터 세트 로딩\n",
    "iris= load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccf28a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris target 값: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "iris target 명: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# 피처만으로 된 데이터 저장 \n",
    "iris_data=iris.data\n",
    "\n",
    "# 레이블(결정 값) 데이터 저장 \n",
    "iris_label=iris.target\n",
    "\n",
    "print('iris target 값:',iris_label)\n",
    "print('iris target 명:',iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccd36923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame으로 변환\n",
    "iris_df=pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
    "iris_df['label']=iris.target #label이라는 변수 추가\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f40332",
   "metadata": {},
   "source": [
    "#### 1. 데이터 세트 분리: 학습용 데이터와 테스트용 데이터 분리 \n",
    "- train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a719e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(iris_data,iris_label,test_size=0.2,random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e080c-52e8-4095-bae9-baf85409d318",
   "metadata": {},
   "source": [
    "#### 2. 모델 학습: 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델 학습\n",
    "- DecisionTreeClassifier: 의사 결정 트리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18b5e557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=11)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clf=DecisionTreeClassifier(random_state=11)\n",
    "\n",
    "#학습용 피처 데이터 학습 수행\n",
    "df_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b0df1a-f621-4a49-abce-e9257f9d9df7",
   "metadata": {},
   "source": [
    "#### 3. 예측 수행: 학습된 ML 알고리즘을 적용해 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c3a1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습이 완료된 DecisionTreeClassifier 객체에 테스트 데이터 세트로 예측 수행\n",
    "pred=df_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff28add-7ee8-4d20-a470-ca1c3984ca61",
   "metadata": {},
   "source": [
    "#### 4. 평가: 예측값과 테스트 데이터의 실제 결과값을 비교하여 ML 모델 성능 평가\n",
    "- accuracy_score: 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be485b68-6d93-42e9-8aaa-421ef3455fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Accuracy classification score.\n",
       "\n",
       "In multilabel classification, this function computes subset accuracy:\n",
       "the set of labels predicted for a sample must *exactly* match the\n",
       "corresponding set of labels in y_true.\n",
       "\n",
       "Read more in the :ref:`User Guide <accuracy_score>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : 1d array-like, or label indicator array / sparse matrix\n",
       "    Ground truth (correct) labels.\n",
       "\n",
       "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
       "    Predicted labels, as returned by a classifier.\n",
       "\n",
       "normalize : bool, default=True\n",
       "    If ``False``, return the number of correctly classified samples.\n",
       "    Otherwise, return the fraction of correctly classified samples.\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "score : float\n",
       "    If ``normalize == True``, return the fraction of correctly\n",
       "    classified samples (float), else returns the number of correctly\n",
       "    classified samples (int).\n",
       "\n",
       "    The best performance is 1 with ``normalize == True`` and the number\n",
       "    of samples with ``normalize == False``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "jaccard_score, hamming_loss, zero_one_loss\n",
       "\n",
       "Notes\n",
       "-----\n",
       "In binary and multiclass classification, this function is equal\n",
       "to the ``jaccard_score`` function.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import accuracy_score\n",
       ">>> y_pred = [0, 2, 1, 3]\n",
       ">>> y_true = [0, 1, 2, 3]\n",
       ">>> accuracy_score(y_true, y_pred)\n",
       "0.5\n",
       ">>> accuracy_score(y_true, y_pred, normalize=False)\n",
       "2\n",
       "\n",
       "In the multilabel case with binary label indicators:\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
       "0.5\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\user\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cf8f5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에측 정확도:0.9333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('에측 정확도:{0:.4f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d432ce8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 03 사이킷런 기반 프레임워크 익히기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31cbf1b-e679-455e-b40b-426f41f392ed",
   "metadata": {},
   "source": [
    "### Estimator 이해 및 fit(),predic() 메서드 \n",
    "- fit(): ML 모델 학습\n",
    "- preidct(): 학습된 모델 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9325f669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "iris_data2= load_iris()\n",
    "print(type(iris_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e97d774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "붓꽃 데이터 세트의 키들: dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "keys=iris_data2.keys()\n",
    "print('붓꽃 데이터 세트의 키들:',keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa20e1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_names 의 type: <class 'list'>\n",
      "feature_names 의 shape: 4\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "target_names 의 type: <class 'numpy.ndarray'>\n",
      "target_names 의 shape: 3\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "data 의 type: <class 'numpy.ndarray'>\n",
      "data 의 shape: (150, 4)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "\n",
      "target 의 type: <class 'numpy.ndarray'>\n",
      "target 의 shape: (150,)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print('\\nfeature_names 의 type:',type(iris_data2.feature_names))\n",
    "print('feature_names 의 shape:',len(iris_data2.feature_names))\n",
    "print(iris_data2.feature_names)\n",
    "\n",
    "print('\\ntarget_names 의 type:',type(iris_data2.target_names))\n",
    "print('target_names 의 shape:',len(iris_data2.target_names))\n",
    "print(iris_data2.target_names)\n",
    "\n",
    "print('\\ndata 의 type:',type(iris_data2.data))\n",
    "print('data 의 shape:',iris_data2.data.shape)\n",
    "print(iris_data2['data'])\n",
    "\n",
    "print('\\ntarget 의 type:',type(iris_data2.target))\n",
    "print('target 의 shape:',iris_data2.target.shape)\n",
    "print(iris_data2.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4dbc2f",
   "metadata": {},
   "source": [
    "## 04 Model Selection 모듈 소개\n",
    "  \n",
    "### 학습/테스트 데이터 세트 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029ed8f-283f-4039-a2a7-01d7e80a4d3d",
   "metadata": {},
   "source": [
    "(학습용 데이터의 피처 데이터 세트, 테스트용 데이터의 피처 데이트 세트, 학습용 데이터의 레이블 데이터 세트, 테스트용 데이터의 레이블 데이터 세트) = train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad06c165-4f6a-41f9-b0f0-b3ad50a3a0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Split arrays or matrices into random train and test subsets\n",
       "\n",
       "Quick utility that wraps input validation and\n",
       "``next(ShuffleSplit().split(X, y))`` and application to input data\n",
       "into a single call for splitting (and optionally subsampling) data in a\n",
       "oneliner.\n",
       "\n",
       "Read more in the :ref:`User Guide <cross_validation>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "*arrays : sequence of indexables with same length / shape[0]\n",
       "    Allowed inputs are lists, numpy arrays, scipy-sparse\n",
       "    matrices or pandas dataframes.\n",
       "\n",
       "test_size : float or int, default=None\n",
       "    If float, should be between 0.0 and 1.0 and represent the proportion\n",
       "    of the dataset to include in the test split. If int, represents the\n",
       "    absolute number of test samples. If None, the value is set to the\n",
       "    complement of the train size. If ``train_size`` is also None, it will\n",
       "    be set to 0.25.\n",
       "\n",
       "train_size : float or int, default=None\n",
       "    If float, should be between 0.0 and 1.0 and represent the\n",
       "    proportion of the dataset to include in the train split. If\n",
       "    int, represents the absolute number of train samples. If None,\n",
       "    the value is automatically set to the complement of the test size.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the shuffling applied to the data before applying the split.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "\n",
       "shuffle : bool, default=True\n",
       "    Whether or not to shuffle the data before splitting. If shuffle=False\n",
       "    then stratify must be None.\n",
       "\n",
       "stratify : array-like, default=None\n",
       "    If not None, data is split in a stratified fashion, using this as\n",
       "    the class labels.\n",
       "    Read more in the :ref:`User Guide <stratification>`.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "splitting : list, length=2 * len(arrays)\n",
       "    List containing train-test split of inputs.\n",
       "\n",
       "    .. versionadded:: 0.16\n",
       "        If the input is sparse, the output will be a\n",
       "        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
       "        input type.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.model_selection import train_test_split\n",
       ">>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
       ">>> X\n",
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])\n",
       ">>> list(y)\n",
       "[0, 1, 2, 3, 4]\n",
       "\n",
       ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
       "...     X, y, test_size=0.33, random_state=42)\n",
       "...\n",
       ">>> X_train\n",
       "array([[4, 5],\n",
       "       [0, 1],\n",
       "       [6, 7]])\n",
       ">>> y_train\n",
       "[2, 0, 3]\n",
       ">>> X_test\n",
       "array([[2, 3],\n",
       "       [8, 9]])\n",
       ">>> y_test\n",
       "[1, 4]\n",
       "\n",
       ">>> train_test_split(y, shuffle=False)\n",
       "[[0, 1, 2], [3, 4]]\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a008197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris=load_iris()\n",
    "dt_clf=DecisionTreeClassifier()\n",
    "train_data=iris.data\n",
    "train_label=iris.target\n",
    "dt_clf.fit(train_data,train_label)\n",
    "\n",
    "pred=dt_clf.predict(train_data)\n",
    "print('예측 정확도:',accuracy_score(train_label,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4061f4-9c8d-47a5-bd64-9456a0557dc5",
   "metadata": {},
   "source": [
    "- 이미 학습한 데이터 세트를 기반으로 예측하였기 때문에 정확도가 100% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6361b0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris=load_iris()\n",
    "dt_clf=DecisionTreeClassifier()\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(iris_data,iris_label,test_size=0.3,random_state=121)\n",
    "dt_clf.fit(X_train,y_train)\n",
    "\n",
    "pred=dt_clf.predict(X_test)\n",
    "print('예측 정확도:',round(accuracy_score(y_test,pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3630b-6858-4fed-9141-1a66046ce3b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 교차 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1b0a4-cbdb-496c-9034-1b2246bc888d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### K 폴드 교차 검증\n",
    "- K개의 데이터 폴드 세트를 만들어서 K번만큼 각 폴드 세트에 학습과 검증 평가를 반복적 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0df881dd-f4fb-44c5-8ef5-cb74835d7a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "K-Folds cross-validator\n",
       "\n",
       "Provides train/test indices to split data in train/test sets. Split\n",
       "dataset into k consecutive folds (without shuffling by default).\n",
       "\n",
       "Each fold is then used once as a validation while the k - 1 remaining\n",
       "folds form the training set.\n",
       "\n",
       "Read more in the :ref:`User Guide <k_fold>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_splits : int, default=5\n",
       "    Number of folds. Must be at least 2.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "        ``n_splits`` default value changed from 3 to 5.\n",
       "\n",
       "shuffle : bool, default=False\n",
       "    Whether to shuffle the data before splitting into batches.\n",
       "    Note that the samples within each split will not be shuffled.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    When `shuffle` is True, `random_state` affects the ordering of the\n",
       "    indices, which controls the randomness of each fold. Otherwise, this\n",
       "    parameter has no effect.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.model_selection import KFold\n",
       ">>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
       ">>> y = np.array([1, 2, 3, 4])\n",
       ">>> kf = KFold(n_splits=2)\n",
       ">>> kf.get_n_splits(X)\n",
       "2\n",
       ">>> print(kf)\n",
       "KFold(n_splits=2, random_state=None, shuffle=False)\n",
       ">>> for train_index, test_index in kf.split(X):\n",
       "...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
       "...     X_train, X_test = X[train_index], X[test_index]\n",
       "...     y_train, y_test = y[train_index], y[test_index]\n",
       "TRAIN: [2 3] TEST: [0 1]\n",
       "TRAIN: [0 1] TEST: [2 3]\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The first ``n_samples % n_splits`` folds have size\n",
       "``n_samples // n_splits + 1``, other folds have size\n",
       "``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
       "\n",
       "Randomized CV splitters may return different results for each call of\n",
       "split. You can make the results identical by setting `random_state`\n",
       "to an integer.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "StratifiedKFold : Takes group information into account to avoid building\n",
       "    folds with imbalanced class distributions (for binary or multiclass\n",
       "    classification tasks).\n",
       "\n",
       "GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
       "\n",
       "RepeatedKFold : Repeats K-Fold n times.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "KFold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "632bb61e-78e6-46da-a456-f8cfecd7c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "붓꽃 데이터 세트 크기: 150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "iris =load_iris()\n",
    "feature=iris.data\n",
    "label=iris.target\n",
    "dt_clf=DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "kfold=KFold(n_splits=5) #5개의 폴드 세트로 분리하는 KFold 객체 생성 shuffle=True를 통해 분포를 고르게 할 수 있다\n",
    "cv_accuracy=[] # 폴드 세트별 정확도를 담을 리스트 객체 생성\n",
    "print('붓꽃 데이터 세트 크기:',feature.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37d5ce43-4c32-4f6d-85c5-1588f12be02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 교차 검증 정확도:1.0, 학습 데이터 크기 120:, 검증데이터 크기:30\n",
      "#1 검증 세트 인덱스:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29]\n",
      "\n",
      "#2 교차 검증 정확도:0.9667, 학습 데이터 크기 120:, 검증데이터 크기:30\n",
      "#2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n",
      " 54 55 56 57 58 59]\n",
      "\n",
      "#3 교차 검증 정확도:0.8667, 학습 데이터 크기 120:, 검증데이터 크기:30\n",
      "#3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n",
      " 84 85 86 87 88 89]\n",
      "\n",
      "#4 교차 검증 정확도:0.9333, 학습 데이터 크기 120:, 검증데이터 크기:30\n",
      "#4 검증 세트 인덱스:[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
      "\n",
      "#5 교차 검증 정확도:0.7333, 학습 데이터 크기 120:, 검증데이터 크기:30\n",
      "#5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "\n",
      "## 평균 검증 정확도: 0.9\n"
     ]
    }
   ],
   "source": [
    "#n_iter : 검증 횟수\n",
    "n_iter=0\n",
    "\n",
    "#KFold 객체의 split()를 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환\n",
    "for train_index, test_index in kfold.split(feature):\n",
    "    #kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 ㅌ스트 데이터 추출\n",
    "    X_train,X_test=feature[train_index],feature[test_index] \n",
    "    y_train,y_test= label[train_index],label[test_index]\n",
    "    #학습\n",
    "    dt_clf.fit(X_train,y_train)\n",
    "    #예측\n",
    "    pred=dt_clf.predict(X_test)\n",
    "    n_iter +=1\n",
    "    #반복 시마다 정확도 측정\n",
    "    accuracy=np.round(accuracy_score(y_test,pred),4)\n",
    "    train_size=X_train.shape[0]\n",
    "    test_size=X_test.shape[0]\n",
    "    print('\\n#{0} 교차 검증 정확도:{1}, 학습 데이터 크기 {2}:, 검증데이터 크기:{3}'.format(n_iter,accuracy,train_size,test_size))\n",
    "    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "\n",
    "#평균 정확도 계산   \n",
    "print('\\n## 평균 검증 정확도:',np.mean(cv_accuracy))\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541a555-c4b5-4a5f-b145-37b8e3144548",
   "metadata": {},
   "source": [
    "#### Stratified K 폴드\n",
    "- 불균형한 분포도를 가진 레이블 데이터 집합을 위한 k 폴드 방식  -> 특정 레이블 값이 특이하게 많거나 적은 경우 \n",
    "- K 폴드의 원본 데이터 집합의 레이블 분포를 학습 및 테스트 세트에 제대로 분배 하지 못하는 문제 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ffd5654a-c886-412e-89b5-9c74c137815c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50\n",
       "1    50\n",
       "2    50\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df=pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['label']=iris.target\n",
    "iris_df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ffc744-8487-47ff-a96b-3a567be160e8",
   "metadata": {},
   "source": [
    "- 3개의 품종 모두 50개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35c7145c-7bf8-4c9a-93b2-5e757d2a55d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 교차 검증:1\n",
      "학습 레이블 데이터 분포:\n",
      " 1    50\n",
      "2    50\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 0    50\n",
      "Name: label, dtype: int64\n",
      "## 교차 검증:2\n",
      "학습 레이블 데이터 분포:\n",
      " 0    50\n",
      "2    50\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 1    50\n",
      "Name: label, dtype: int64\n",
      "## 교차 검증:3\n",
      "학습 레이블 데이터 분포:\n",
      " 0    50\n",
      "1    50\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 2    50\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "kfold=KFold(n_splits=3)\n",
    "n_iter=0\n",
    "\n",
    "for train_index, test_index in kfold.split(iris_df):\n",
    "    n_iter +=1\n",
    "    label_train=iris_df['label'].iloc[train_index]\n",
    "    label_test=iris_df['label'].iloc[test_index]\n",
    "    print('## 교차 검증:{0}'.format(n_iter))\n",
    "    print('학습 레이블 데이터 분포:\\n',label_train.value_counts())\n",
    "    print('검증 레이블 데이터 분포:\\n',label_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8f31586-493d-4ec8-9c6d-ed7afa4b6835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 교차 검증:1\n",
      "학습 레이블 데이터 분포:\n",
      " 1    37\n",
      "2    33\n",
      "0    30\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 0    20\n",
      "2    17\n",
      "1    13\n",
      "Name: label, dtype: int64\n",
      "## 교차 검증:2\n",
      "학습 레이블 데이터 분포:\n",
      " 0    36\n",
      "1    34\n",
      "2    30\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 2    20\n",
      "1    16\n",
      "0    14\n",
      "Name: label, dtype: int64\n",
      "## 교차 검증:3\n",
      "학습 레이블 데이터 분포:\n",
      " 2    37\n",
      "0    34\n",
      "1    29\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 1    21\n",
      "0    16\n",
      "2    13\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "kfold=KFold(n_splits=3,shuffle=True)\n",
    "n_iter=0\n",
    "\n",
    "for train_index, test_index in kfold.split(iris_df):\n",
    "    n_iter +=1\n",
    "    label_train=iris_df['label'].iloc[train_index]\n",
    "    label_test=iris_df['label'].iloc[test_index]\n",
    "    print('## 교차 검증:{0}'.format(n_iter))\n",
    "    print('학습 레이블 데이터 분포:\\n',label_train.value_counts())\n",
    "    print('검증 레이블 데이터 분포:\\n',label_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "864424e4-d494-4e4e-a899-d6b914fec9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 교차 검증:1\n",
      "학습 레이블 데이터 분포:\n",
      " 2    34\n",
      "0    33\n",
      "1    33\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 0    17\n",
      "1    17\n",
      "2    16\n",
      "Name: label, dtype: int64\n",
      "## 교차 검증:2\n",
      "학습 레이블 데이터 분포:\n",
      " 1    34\n",
      "0    33\n",
      "2    33\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 0    17\n",
      "2    17\n",
      "1    16\n",
      "Name: label, dtype: int64\n",
      "## 교차 검증:3\n",
      "학습 레이블 데이터 분포:\n",
      " 0    34\n",
      "1    33\n",
      "2    33\n",
      "Name: label, dtype: int64\n",
      "검증 레이블 데이터 분포:\n",
      " 1    17\n",
      "2    17\n",
      "0    16\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3)\n",
    "n_iter=0\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(iris_df,iris_df['label']):\n",
    "    n_iter += 1\n",
    "    label_train=iris_df['label'].iloc[train_index]\n",
    "    label_test=iris_df['label'].iloc[test_index]\n",
    "    print('## 교차 검증:{0}'.format(n_iter))\n",
    "    print('학습 레이블 데이터 분포:\\n',label_train.value_counts())\n",
    "    print('검증 레이블 데이터 분포:\\n',label_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddded29a-325a-4d6b-8b3a-e11dcee74fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 교차 검증 정확도:0.98, 학습 데이터 크기 100:, 검증데이터 크기:50\n",
      "#1 검증 세트 인덱스:[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  50\n",
      "  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 100 101\n",
      " 102 103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
      "\n",
      "#2 교차 검증 정확도:0.94, 학습 데이터 크기 100:, 검증데이터 크기:50\n",
      "#2 검증 세트 인덱스:[ 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  67\n",
      "  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132]\n",
      "\n",
      "#3 교차 검증 정확도:0.98, 학습 데이터 크기 100:, 검증데이터 크기:50\n",
      "#3 검증 세트 인덱스:[ 34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  83  84\n",
      "  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 133 134 135\n",
      " 136 137 138 139 140 141 142 143 144 145 146 147 148 149]\n",
      "\n",
      "## 교차 검증별 정확도: [0.98 0.94 0.98]\n",
      "\n",
      "## 평균 검증 정확도: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "dt_clf=DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "skfold=StratifiedKFold(n_splits=3)\n",
    "n_iter=0\n",
    "cv_accuracy=[]\n",
    "\n",
    "for train_index, test_index in skfold.split(feature,label): #StratifiedKFold의 split() 호출시 반드시 레이블 데이터 세트 추가 입력 필요\n",
    "    X_train,X_test=feature[train_index],feature[test_index]\n",
    "    y_train,y_test= label[train_index],label[test_index]\n",
    "    dt_clf.fit(X_train,y_train)\n",
    "    pred=dt_clf.predict(X_test)\n",
    "    n_iter += 1\n",
    "    accuracy=np.round(accuracy_score(y_test,pred),4)\n",
    "    train_size=X_train.shape[0]\n",
    "    test_size=X_test.shape[0]\n",
    "    print('\\n#{0} 교차 검증 정확도:{1}, 학습 데이터 크기 {2}:, 검증데이터 크기:{3}'.format(n_iter,accuracy,train_size,test_size))\n",
    "    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))\n",
    "    cv_accuracy.append(accuracy)\n",
    "\n",
    "print('\\n## 교차 검증별 정확도:',np.round(cv_accuracy,4))    \n",
    "print('\\n## 평균 검증 정확도:',np.mean(cv_accuracy))\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88241b2-15b6-4a3c-9c53-fb5da6b45702",
   "metadata": {},
   "source": [
    "#### cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "82a9d90f-c0e8-4a20-af71-c73d50ae40c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.model_selection._validation.cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "155e5732-5b24-4566-95f4-e312b5d01d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증별 정확도: [0.98 0.94 0.98]\n",
      "평균 검증별 정확도; 0.9667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score,cross_validate\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data=load_iris()\n",
    "dt_clf=DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "data= iris_data.data\n",
    "label=iris_data.target\n",
    "\n",
    "scores = cross_val_score(dt_clf, data, label, scoring='accuracy',cv=3)\n",
    "print('교차 검증별 정확도:', np.round(scores,4))\n",
    "print('평균 검증별 정확도;',np.round(np.mean(scores),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dffe87-6108-4a62-8f0c-036399eccf1a",
   "metadata": {},
   "source": [
    "### GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝 한 번에 \n",
    "- 하이퍼 파라미터: 머신러닝 알고리즘을 구성하는 주요 구성 요소, -> 조정을 통해 알고리즘 예측 성능 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9e516b95-a130-4860-b745-ca00a90d6096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrefit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2*n_jobs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0merror_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Exhaustive search over specified parameter values for an estimator.\n",
       "\n",
       "Important members are fit, predict.\n",
       "\n",
       "GridSearchCV implements a \"fit\" and a \"score\" method.\n",
       "It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
       "\"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
       "implemented in the estimator used.\n",
       "\n",
       "The parameters of the estimator used to apply these methods are optimized\n",
       "by cross-validated grid-search over a parameter grid.\n",
       "\n",
       "Read more in the :ref:`User Guide <grid_search>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "estimator : estimator object.\n",
       "    This is assumed to implement the scikit-learn estimator interface.\n",
       "    Either estimator needs to provide a ``score`` function,\n",
       "    or ``scoring`` must be passed.\n",
       "\n",
       "param_grid : dict or list of dictionaries\n",
       "    Dictionary with parameters names (`str`) as keys and lists of\n",
       "    parameter settings to try as values, or a list of such\n",
       "    dictionaries, in which case the grids spanned by each dictionary\n",
       "    in the list are explored. This enables searching over any sequence\n",
       "    of parameter settings.\n",
       "\n",
       "scoring : str, callable, list, tuple or dict, default=None\n",
       "    Strategy to evaluate the performance of the cross-validated model on\n",
       "    the test set.\n",
       "\n",
       "    If `scoring` represents a single score, one can use:\n",
       "\n",
       "    - a single string (see :ref:`scoring_parameter`);\n",
       "    - a callable (see :ref:`scoring`) that returns a single value.\n",
       "\n",
       "    If `scoring` represents multiple scores, one can use:\n",
       "\n",
       "    - a list or tuple of unique strings;\n",
       "    - a callable returning a dictionary where the keys are the metric\n",
       "      names and the values are the metric scores;\n",
       "    - a dictionary with metric names as keys and callables a values.\n",
       "\n",
       "    See :ref:`multimetric_grid_search` for an example.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    Number of jobs to run in parallel.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "    .. versionchanged:: v0.20\n",
       "       `n_jobs` default changed from 1 to None\n",
       "\n",
       "refit : bool, str, or callable, default=True\n",
       "    Refit an estimator using the best found parameters on the whole\n",
       "    dataset.\n",
       "\n",
       "    For multiple metric evaluation, this needs to be a `str` denoting the\n",
       "    scorer that would be used to find the best parameters for refitting\n",
       "    the estimator at the end.\n",
       "\n",
       "    Where there are considerations other than maximum score in\n",
       "    choosing a best estimator, ``refit`` can be set to a function which\n",
       "    returns the selected ``best_index_`` given ``cv_results_``. In that\n",
       "    case, the ``best_estimator_`` and ``best_params_`` will be set\n",
       "    according to the returned ``best_index_`` while the ``best_score_``\n",
       "    attribute will not be available.\n",
       "\n",
       "    The refitted estimator is made available at the ``best_estimator_``\n",
       "    attribute and permits using ``predict`` directly on this\n",
       "    ``GridSearchCV`` instance.\n",
       "\n",
       "    Also for multiple metric evaluation, the attributes ``best_index_``,\n",
       "    ``best_score_`` and ``best_params_`` will only be available if\n",
       "    ``refit`` is set and all of them will be determined w.r.t this specific\n",
       "    scorer.\n",
       "\n",
       "    See ``scoring`` parameter to know more about multiple metric\n",
       "    evaluation.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "        Support for callable added.\n",
       "\n",
       "cv : int, cross-validation generator or an iterable, default=None\n",
       "    Determines the cross-validation splitting strategy.\n",
       "    Possible inputs for cv are:\n",
       "\n",
       "    - None, to use the default 5-fold cross validation,\n",
       "    - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
       "    - :term:`CV splitter`,\n",
       "    - An iterable yielding (train, test) splits as arrays of indices.\n",
       "\n",
       "    For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
       "    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
       "    other cases, :class:`KFold` is used. These splitters are instantiated\n",
       "    with `shuffle=False` so the splits will be the same across calls.\n",
       "\n",
       "    Refer :ref:`User Guide <cross_validation>` for the various\n",
       "    cross-validation strategies that can be used here.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "        ``cv`` default value if None changed from 3-fold to 5-fold.\n",
       "\n",
       "verbose : int\n",
       "    Controls the verbosity: the higher, the more messages.\n",
       "\n",
       "    - >1 : the computation time for each fold and parameter candidate is\n",
       "      displayed;\n",
       "    - >2 : the score is also displayed;\n",
       "    - >3 : the fold and candidate parameter indexes are also displayed\n",
       "      together with the starting time of the computation.\n",
       "\n",
       "pre_dispatch : int, or str, default=n_jobs\n",
       "    Controls the number of jobs that get dispatched during parallel\n",
       "    execution. Reducing this number can be useful to avoid an\n",
       "    explosion of memory consumption when more jobs get dispatched\n",
       "    than CPUs can process. This parameter can be:\n",
       "\n",
       "        - None, in which case all the jobs are immediately\n",
       "          created and spawned. Use this for lightweight and\n",
       "          fast-running jobs, to avoid delays due to on-demand\n",
       "          spawning of the jobs\n",
       "\n",
       "        - An int, giving the exact number of total jobs that are\n",
       "          spawned\n",
       "\n",
       "        - A str, giving an expression as a function of n_jobs,\n",
       "          as in '2*n_jobs'\n",
       "\n",
       "error_score : 'raise' or numeric, default=np.nan\n",
       "    Value to assign to the score if an error occurs in estimator fitting.\n",
       "    If set to 'raise', the error is raised. If a numeric value is given,\n",
       "    FitFailedWarning is raised. This parameter does not affect the refit\n",
       "    step, which will always raise the error.\n",
       "\n",
       "return_train_score : bool, default=False\n",
       "    If ``False``, the ``cv_results_`` attribute will not include training\n",
       "    scores.\n",
       "    Computing training scores is used to get insights on how different\n",
       "    parameter settings impact the overfitting/underfitting trade-off.\n",
       "    However computing the scores on the training set can be computationally\n",
       "    expensive and is not strictly required to select the parameters that\n",
       "    yield the best generalization performance.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "    .. versionchanged:: 0.21\n",
       "        Default value was changed from ``True`` to ``False``\n",
       "\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn import svm, datasets\n",
       ">>> from sklearn.model_selection import GridSearchCV\n",
       ">>> iris = datasets.load_iris()\n",
       ">>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
       ">>> svc = svm.SVC()\n",
       ">>> clf = GridSearchCV(svc, parameters)\n",
       ">>> clf.fit(iris.data, iris.target)\n",
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
       ">>> sorted(clf.cv_results_.keys())\n",
       "['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
       " 'param_C', 'param_kernel', 'params',...\n",
       " 'rank_test_score', 'split0_test_score',...\n",
       " 'split2_test_score', ...\n",
       " 'std_fit_time', 'std_score_time', 'std_test_score']\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "cv_results_ : dict of numpy (masked) ndarrays\n",
       "    A dict with keys as column headers and values as columns, that can be\n",
       "    imported into a pandas ``DataFrame``.\n",
       "\n",
       "    For instance the below given table\n",
       "\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
       "    +============+===========+============+=================+===+=========+\n",
       "    |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "    |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
       "    +------------+-----------+------------+-----------------+---+---------+\n",
       "\n",
       "    will be represented by a ``cv_results_`` dict of::\n",
       "\n",
       "        {\n",
       "        'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
       "                                     mask = [False False False False]...)\n",
       "        'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
       "                                    mask = [ True  True False False]...),\n",
       "        'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
       "                                     mask = [False False  True  True]...),\n",
       "        'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
       "        'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
       "        'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
       "        'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
       "        'rank_test_score'    : [2, 4, 3, 1],\n",
       "        'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
       "        'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
       "        'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
       "        'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
       "        'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
       "        'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
       "        'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
       "        'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
       "        'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
       "        }\n",
       "\n",
       "    NOTE\n",
       "\n",
       "    The key ``'params'`` is used to store a list of parameter\n",
       "    settings dicts for all the parameter candidates.\n",
       "\n",
       "    The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
       "    ``std_score_time`` are all in seconds.\n",
       "\n",
       "    For multi-metric evaluation, the scores for all the scorers are\n",
       "    available in the ``cv_results_`` dict at the keys ending with that\n",
       "    scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
       "    above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
       "\n",
       "best_estimator_ : estimator\n",
       "    Estimator that was chosen by the search, i.e. estimator\n",
       "    which gave highest score (or smallest loss if specified)\n",
       "    on the left out data. Not available if ``refit=False``.\n",
       "\n",
       "    See ``refit`` parameter for more information on allowed values.\n",
       "\n",
       "best_score_ : float\n",
       "    Mean cross-validated score of the best_estimator\n",
       "\n",
       "    For multi-metric evaluation, this is present only if ``refit`` is\n",
       "    specified.\n",
       "\n",
       "    This attribute is not available if ``refit`` is a function.\n",
       "\n",
       "best_params_ : dict\n",
       "    Parameter setting that gave the best results on the hold out data.\n",
       "\n",
       "    For multi-metric evaluation, this is present only if ``refit`` is\n",
       "    specified.\n",
       "\n",
       "best_index_ : int\n",
       "    The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
       "    candidate parameter setting.\n",
       "\n",
       "    The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
       "    the parameter setting for the best model, that gives the highest\n",
       "    mean score (``search.best_score_``).\n",
       "\n",
       "    For multi-metric evaluation, this is present only if ``refit`` is\n",
       "    specified.\n",
       "\n",
       "scorer_ : function or a dict\n",
       "    Scorer function used on the held out data to choose the best\n",
       "    parameters for the model.\n",
       "\n",
       "    For multi-metric evaluation, this attribute holds the validated\n",
       "    ``scoring`` dict which maps the scorer key to the scorer callable.\n",
       "\n",
       "n_splits_ : int\n",
       "    The number of cross-validation splits (folds/iterations).\n",
       "\n",
       "refit_time_ : float\n",
       "    Seconds used for refitting the best model on the whole dataset.\n",
       "\n",
       "    This is present only if ``refit`` is not False.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "multimetric_ : bool\n",
       "    Whether or not the scorers compute several metrics.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The parameters selected are those that maximize the score of the left out\n",
       "data, unless an explicit score is passed in which case it is used instead.\n",
       "\n",
       "If `n_jobs` was set to a value higher than one, the data is copied for each\n",
       "point in the grid (and not `n_jobs` times). This is done for efficiency\n",
       "reasons if individual jobs take very little time, but may raise errors if\n",
       "the dataset is large and not enough memory is available.  A workaround in\n",
       "this case is to set `pre_dispatch`. Then, the memory is copied only\n",
       "`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
       "n_jobs`.\n",
       "\n",
       "See Also\n",
       "---------\n",
       "ParameterGrid : Generates all the combinations of a hyperparameter grid.\n",
       "train_test_split : Utility function to split the data into a development\n",
       "    set usable for fitting a GridSearchCV instance and an evaluation set\n",
       "    for its final evaluation.\n",
       "sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
       "    loss function.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GridSearchCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "562d4313-86fe-4594-a1a0-158398ff380d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': [1, 2, 3], 'min_samples_split': [2, 3]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_parameters={'max_depth':[1,2,3],\n",
    "                 'min_samples_split':[2,3]\n",
    "                }\n",
    "grid_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c35d263-ac2d-47e7-a9a3-337124203667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris_data = load_iris()\n",
    "X_train,X_test,y_train,y_test = train_test_split(iris_data.data,iris_data.target,test_size=0.2,random_state=121)\n",
    "\n",
    "dtree=DecisionTreeClassifier()\n",
    "\n",
    "parameters={'max_depth':[1,2,3],'min_samples_split':[2,3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79abb575-af38-49db-a8b9-b00ecb4d83fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 3}</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 3}</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>3</td>\n",
       "      <td>0.925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 2}</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 3}</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     params  mean_test_score  rank_test_score  \\\n",
       "0  {'max_depth': 1, 'min_samples_split': 2}         0.700000                5   \n",
       "1  {'max_depth': 1, 'min_samples_split': 3}         0.700000                5   \n",
       "2  {'max_depth': 2, 'min_samples_split': 2}         0.958333                3   \n",
       "3  {'max_depth': 2, 'min_samples_split': 3}         0.958333                3   \n",
       "4  {'max_depth': 3, 'min_samples_split': 2}         0.975000                1   \n",
       "5  {'max_depth': 3, 'min_samples_split': 3}         0.975000                1   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  \n",
       "0              0.700                0.7               0.70  \n",
       "1              0.700                0.7               0.70  \n",
       "2              0.925                1.0               0.95  \n",
       "3              0.925                1.0               0.95  \n",
       "4              0.975                1.0               0.95  \n",
       "5              0.975                1.0               0.95  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_dtree =GridSearchCV(dtree,param_grid=parameters,cv=3,refit=True)\n",
    "grid_dtree.fit(X_train, y_train)\n",
    "\n",
    "scores_df= pd.DataFrame(grid_dtree.cv_results_)\n",
    "scores_df[['params','mean_test_score','rank_test_score','split0_test_score','split1_test_score','split2_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd3e6c3e-96a5-415f-aab2-b89444e8dcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV 최적 파라미터: {'max_depth': 3, 'min_samples_split': 2}\n",
      "GridSearchCV 최고 정확도:0.9750\n"
     ]
    }
   ],
   "source": [
    "print('GridSearchCV 최적 파라미터:',grid_dtree.best_params_)\n",
    "print('GridSearchCV 최고 정확도:{0:.4f}'.format(grid_dtree.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49e67d28-0ab6-44e8-aa7b-a0e71037d0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 세트 정확도:0.9667\n"
     ]
    }
   ],
   "source": [
    "estimator=grid_dtree.best_estimator_\n",
    "\n",
    "pred=estimator.predict(X_test)\n",
    "print(\"테스트 데이터 세트 정확도:{0:.4f}\".format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b3daa-8b4d-4e1c-a56c-76175297c101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
